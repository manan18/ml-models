# # -*- coding: utf-8 -*-
# """Top_Performer.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1X_WTrA6V7rR8lrCWb1y97LA5MYHx7b7E
# """

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression  # Example model (replace if needed)
from sklearn.preprocessing import LabelEncoder, OneHotEncoder  # For handling categorical data (if needed)
from sklearn.metrics import accuracy_score

# Set random seed for reproducibility
np.random.seed(0)

# Define parameters for order simulation
# num_orders = 1000
# employees = ['Alice', 'Bob', 'Charlie', 'David', 'Eve']
# statuses = [True, False]  # Approval status options

# # Generate random dates within a year range
# date_range = pd.date_range(start='2022-01-01', end='2022-12-31', periods=num_orders)
# random_dates = np.random.choice(date_range, size=num_orders, replace=True)

# start_dates = np.random.choice(date_range, size=num_orders, replace=True)
# start_dates = pd.to_datetime(start_dates).date  # Convert to datetime and extract date component

# # Generate random end dates by adding random days to start dates
# random_days = np.random.randint(1, 10, num_orders)  # Random days to add
# end_dates = [start_date + pd.Timedelta(days=random_day) for start_date, random_day in zip(start_dates, random_days)]

# data = {
#     'OrderID': range(1, num_orders + 1),
#     'Price': np.random.uniform(100, 1000, num_orders).round(2),
#     'Employee': np.random.choice(employees, num_orders),
#     'Date': random_dates,
#     'Approved': np.random.choice(statuses, num_orders, p=[0.7, 0.3]),  # 70% chance of approval
#     'Start_Date': start_dates,
#     'End_Date': end_dates
# }

# df = pd.DataFrame(data)

# # Display the DataFrame with start and end dates
# # print(df.head())

# # print(df)

# df['Total_Orders'] = df.groupby('Employee')['OrderID'].transform('count')

# # Sum approved orders per employee
# df['Approved_Orders'] = df.groupby('Employee')['Approved'].transform('sum')

# # Calculate the duration for each order
# df['Start_Date'] = pd.to_datetime(df['Start_Date'])
# df['End_Date'] = pd.to_datetime(df['End_Date'])
# df['Duration'] = (df['End_Date'] - df['Start_Date']).dt.days
# df['Percentage'] = (df['Approved_Orders']/df['Total_Orders'])*100

# print(df)

# # Define factor weights
# factor_weights = {
#     'Percentage': 3,
#     'Duration': 1,
#     'Price': 2,
# }

# # Define function to calculate weighted score
# def calculate_weighted_score(row):
#     score = 0
#     for factor, weight in factor_weights.items():
#         if pd.notna(row[factor]):
#             score += row[factor] * weight

#     return score
# # def calculate_percentage(row):
# #     percentage=0
# #     if pd.notna(row['Approved_Orders']):
# #         if pd.notna(row['Total_Orders']):
# #             percentage=(row['Approved_Orders']/row['Total_Orders'])*100

# #     return percentage

# # Apply function to calculate weighted score for each row
# df['Weighted_Score'] = df.apply(calculate_weighted_score, axis=1)
# # df['Percentage_Score']=df.apply(calculate_percentage, axis=1)

# # Display DataFrame with the calculated weighted scores
# # print(df)



# # def calculate_weighted_score(employee_data):
# #     score = 0
# #     for factor, weight in factor_weights.items():
# #         score += employee_data[factor] * weight
# #     # Create Series with weighted score and employee name as index
# #     return pd.Series({'Weighted_Score': score}, index=[employee_data['Employee']])

# # # Top performers by total weighted score (all orders)
# # top_employees_by_total = df.groupby('Employee')['Weighted_Score'].sum().sort_values(ascending=False)
# # print("\nTop Employee(s) by Total Weighted Score:")
# # for employee, score in top_employees_by_total.head().items():  # Display top 5 employees
# #   print(f"- {employee}: Total Weighted Score = {score:.2f}")

# # Find the row with the highest weighted score
# top_performer_row = df.nlargest(1, 'Weighted_Score')  # nlargest selects top n rows

# # Extract details of the top performer
# top_performer_name = top_performer_row['Employee'].values[0]
# top_performer_score = top_performer_row['Weighted_Score'].values[0]
# top_performer_order_id = top_performer_row['OrderID'].values[0]  # Assuming order ID exists

# print(f"Top Performer: {top_performer_name} (Order {top_performer_order_id}) - Weighted Score: {top_performer_score:.2f}")

# # Select relevant features for training the model
# features = ['Percentage', 'Duration', 'Price']

# # Define the target variable
# target = 'Weighted_Score'

# # Split the data into features (X) and target variable (y)
# X = df[features].values
# y = df[target].values

# # Split data into train and test sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
# from tensorflow import keras
# from keras.models import Sequential
# from keras.layers import LSTM, Dense

import pickle

# # # Standardize the data
# scaler = StandardScaler()
# with open('scaler.pkl', 'rb') as f:
#     scaler = pickle.load(f)
X_new_data = {
    'Percentage': [0.75, 0.60, 0.80],  # Example values for Percentage feature
    'Duration': [5, 7, 4],              # Example values for Duration feature
    'Price': [200, 300, 250],           # Example values for Price feature
    'Employee': ['Alice', 'Bob', 'Charlie']  # Example values for Employee feature
}
# X_test_scaled = scaler.transform(X_new_data)
# # Reshape data for LSTM input (assuming you want to consider all features for each sample)
# X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
# X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# # Define the LSTM model
# model = Sequential([
#     LSTM(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),
#     LSTM(32, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),
#     Dense(1)
# ])

# # Compile the model
# model.compile(optimizer='adam', loss='mse')

# # Train the model
# model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32, validation_data=(X_test_reshaped, y_test))

# with open('employee_model.pkl', 'wb') as f:
#     pickle.dump(model, f)


import keras
# def predict1(data):
# model = keras.models.load_model('new_model.h5')
# # with open('employee_model.pkl', 'rb') as f:
# #     model = pickle.load(f)
# #     # Load the data
# X_new = pd.DataFrame(X_new_data)
# X_new_features = X_new.drop(columns=['Employee'])
# X_new_scaled = scaler.transform(X_new_features)
# X_new_reshaped = X_new_scaled.reshape((X_new_scaled.shape[0], 1, X_new_scaled.shape[1]))
# predictions = model.predict(X_new_reshaped)
# predictions_with_employee = np.hstack((predictions, np.array(X_new_data['Employee']).reshape(-1, 1)))
# sorted_predictions = predictions_with_employee[np.argsort(predictions_with_employee[:, 0])]
# top_performer = sorted_predictions[-1]
# print( top_performer)




# # Create a DataFrame for X_new
# X_new = pd.DataFrame(X_new_data)

# # Drop the 'Employee' column as it's not used for predictions
# X_new_features = X_new.drop(columns=['Employee'])

# # Preprocess, scale, and reshape the new data
# # Assuming you've already defined the scaler object and reshaped the data for LSTM
# X_new_scaled = scaler.transform(X_new_features)
# X_new_reshaped = X_new_scaled.reshape((X_new_scaled.shape[0], 1, X_new_scaled.shape[1]))

# # Make predictions using the trained model
# predictions = model.predict(X_new_reshaped)

# # Print the predictions
# predictions_with_employee = np.hstack((predictions, np.array(X_new_data['Employee']).reshape(-1, 1)))

# print(predictions_with_employee)

# # Sort predictions by score (assuming score is in the first column)
# sorted_predictions = predictions_with_employee[np.argsort(predictions_with_employee[:, 0])]

# # Extract the employee with the highest score
# top_performer = sorted_predictions[-1]

# # Print the employee with the highest score
# # print("Top Performer:", top_performer)

def predict1(data):
    model = keras.models.load_model('models/new_model.h5')
    with open('models/scaler.pkl', 'rb') as f:
        scaler = pickle.load(f)
# with open('employee_model.pkl', 'rb') as f:
#     model = pickle.load(f)
#     # Load the data
    X_new = pd.DataFrame(data)
    # print(type(data))
    # print(type(X_new_data))
    # print(type(X_new))
    X_new_features = X_new.drop(columns=['Employee'])
    X_new_scaled = scaler.transform(X_new_features)
    X_new_reshaped = X_new_scaled.reshape((X_new_scaled.shape[0], 1, X_new_scaled.shape[1]))
    predictions = model.predict(X_new_reshaped)
    predictions_with_employee = np.hstack((predictions, np.array(data['Employee']).reshape(-1, 1)))
    sorted_predictions = predictions_with_employee[np.argsort(predictions_with_employee[:, 0])]
    top_performer = sorted_predictions[-1]
    top_performer = top_performer.tolist()
    return  top_performer
